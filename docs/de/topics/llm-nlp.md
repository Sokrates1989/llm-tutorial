# ğŸ§  VerstÃ¤ndnis von LLMs & NLP â€“ Grundlagen fÃ¼r angewandte KI

Dieses README bietet einen grundlegenden Ãœberblick Ã¼ber Large Language Models (LLMs), Natural Language Processing (NLP) und die zentralen Konzepte, die notwendig sind, um moderne KI-Systeme in der Praxis zu verstehen und anzuwenden.

## Inhaltsverzeichnis

1. [ğŸ“– Ãœberblick](#-Ã¼berblick)  
2. [ğŸ§‘â€ğŸ’» Verwendung](#-verwendung)  
3. [ğŸ› ï¸ Konfiguration](#-konfiguration)  
4. [ğŸ§­ Zentrale Konzepte](#-zentrale-konzepte)  
   - [ğŸ”¡ NLP & Machine Learning](#-nlp--machine-learning)  
   - [ğŸ§© Tokenisierung](#-tokenisierung)  
   - [ğŸ§  Transformer & Attention](#-transformer--attention)  
   - [ğŸ“¦ Hugging-Face-Ã–kosystem](#-hugging-face-Ã¶kosystem)  
   - [ğŸ” Training vs. Inferenz](#-training-vs-inferenz)  
   - [ğŸ§  Modelltypen: GPT, BERT, T5, LLaMA](#-modelltypen-gpt-bert-t5-llama)  
   - [ğŸ“¤ Dekodiermethoden](#-dekodiermethoden)  
5. [ğŸš€ Zusammenfassung](#-zusammenfassung)

<br><br>

# ğŸ“– Ãœberblick

Diese Anleitung vermittelt ein tiefes, gleichzeitig praxisnahes VerstÃ¤ndnis fÃ¼r die Bausteine moderner KI-Systeme â€“ insbesondere fÃ¼r LLMs wie GPT, BERT, T5 und LLaMA. Diese Konzepte sind essenziell, wenn man solche Modelle nutzen, feinjustieren oder sogar eigene Anwendungen wie Datenextraktion, Zusammenfassung oder Chatbots entwickeln mÃ¶chte.

<br><br>

# ğŸ§‘â€ğŸ’» Verwendung

Ein vortrainiertes Modell wie BART kann zur Textzusammenfassung genutzt werden â€“ mit der `transformers`-Bibliothek von Hugging Face:

;;;python
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
text = "Germany is a federal republic with 16 states..."
summary = summarizer(text, max_length=60, min_length=15, do_sample=False)
print(summary[0]['summary_text'])
;;;

<br><br>

# ğŸ› ï¸ Konfiguration

Installation der AbhÃ¤ngigkeiten mit Poetry:

;;;bash
poetry add transformers torch
;;;

Optional: Containerisierung mit Dockerfile basierend auf Python 3.10 und notwendigen Systembibliotheken fÃ¼r PyTorch und Hugging Face.

<br><br>

# ğŸ§­ Zentrale Konzepte

## ğŸ”¡ NLP & Machine Learning

**NLP (Natural Language Processing)** ist das Teilgebiet der KI, das sich mit dem VerstÃ¤ndnis, der Interpretation und Generierung menschlicher Sprache durch Maschinen befasst.

**ML (Machine Learning)** ermÃ¶glicht es Modellen, aus Daten zu lernen, anstatt manuell programmiert zu werden. LLMs sind fortgeschrittene ML-Modelle, die auf Sprache spezialisiert sind.

## ğŸ§© Tokenisierung

Tokenisierung beschreibt das Zerlegen von Text in â€Tokensâ€œ â€“ kleine Einheiten, die ein Modell verstehen kann. Beispiele:

- "dog" â†’ 1 Token  
- "doghouse" â†’ 2 Tokens wie `["dog", "house"]`

Jedes Modell verwendet eine eigene Tokenisierungsmethode (z.â€¯B. BPE, WordPiece, SentencePiece).

## ğŸ§  Transformer & Attention

Transformer bilden das Architekturfundament aller modernen LLMs.

**Zentrale Komponente: Attention** â€“ das Modell entscheidet, auf welche WÃ¶rter es sich â€konzentriertâ€œ. Das ist entscheidend fÃ¼r das SprachverstÃ¤ndnis und die Kontextverarbeitung (z.â€¯B. beim AuflÃ¶sen von "er", "es", "dieses").

## ğŸ“¦ Hugging-Face-Ã–kosystem

- `transformers`: Bibliothek fÃ¼r vortrainierte Modelle  
- `datasets`: Sammlung tausender Open-Source-DatensÃ¤tze  
- `evaluate`: Tools zur Bewertung von Modellergebnissen (z.â€¯B. BLEU, ROUGE)  
- `spaces`: Live-Demos fÃ¼r Modelle mit Gradio oder Streamlit  

## ğŸ” Training vs. Inferenz

- **Training**: Das Modell lernt Muster aus Daten (aufwendig, benÃ¶tigt GPUs und viele Daten)  
- **Inferenz**: Das trainierte Modell wird auf neue Eingaben angewendet (z.â€¯B. Textzusammenfassung)  

## ğŸ§  Modelltypen: GPT, BERT, T5, LLaMA

| Modell | Typ             | Zweck                 | Anwendungsbeispiele         |
|--------|------------------|------------------------|------------------------------|
| GPT    | Nur Decoder       | Textgenerierung        | Chatbots, kreative Texte     |
| BERT   | Nur Encoder       | TextverstÃ¤ndnis        | Klassifikation, Suche        |
| T5     | Encoder-Decoder   | Text-zu-Text-Aufgaben  | Ãœbersetzung, Zusammenfassung |
| LLaMA  | Nur Decoder       | Effiziente Generierung | Offline-/Edge-Anwendungen    |

## ğŸ“¤ Dekodiermethoden

Bei der Textgenerierung sagt das Modell Wort fÃ¼r Wort voraus, basierend auf Wahrscheinlichkeiten:

| Methode        | Beschreibung |
|----------------|--------------|
| **Greedy**     | Immer das wahrscheinlichste nÃ¤chste Token wÃ¤hlen |
| **Beam Search**| Mehrere Kandidatensequenzen explorieren und die beste wÃ¤hlen |
| **Sampling**   | ZufÃ¤llige Auswahl unter den wahrscheinlichsten Tokens (fÃ¼r mehr KreativitÃ¤t) |

<br><br>

# ğŸš€ Zusammenfassung

âœ… **Vortrainierte LLMs kÃ¶nnen Ã¼ber Hugging Face geladen und genutzt werden**  
âœ… **VerstÃ¤ndnis fÃ¼r Tokenisierung und Textgenerierung vorhanden**  
âœ… **Kenntnis Ã¼ber Unterschiede gÃ¤ngiger Modellarchitekturen (GPT, BERT usw.)**  
âœ… **Einfluss von Inferenz- und Dekodiermethoden ist bekannt**  
âœ… **Grundlagen fÃ¼r Feintuning, RAG oder eigene Deployments sind gelegt**
