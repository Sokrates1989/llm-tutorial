# ğŸ§  LLM Summarizer

A lightweight Python tool for summarizing natural language text using a pre-trained transformer model from Hugging Face. Ideal for developers and researchers who want quick, high-quality text summarization via CLI or inside a container.

## Table of Contents

1. [ğŸ“– Overview](#-overview)  
2. [ğŸ§‘â€ğŸ’» Usage](#-usage)  
3. [ğŸ› ï¸ Configuration](#-configuration)  
4. [ğŸ“¦ Installation](#-installation)  
   - [ğŸ³ Docker](#-docker)  
   - [ğŸ Poetry + Python](#-poetry--python)  
5. [ğŸ“š Model Details](#-model-details)  
6. [ğŸš€ Summary](#-summary)  

<br>

# ğŸ“– Overview

**LLM Summarizer** loads a pre-trained text summarization model (BART) using Hugging Face's `transformers` library. It simplifies the use of powerful language models by wrapping the summarization functionality in a command-line runnable script and optionally a Docker container.

<br>

# ğŸ§‘â€ğŸ’» Usage

Run the Python script via Poetry:

```bash
poetry run python summarizer/main.py
```

Expected output:

```text
Zusammenfassung:
 Deutschland ist eine fÃ¶derale Republik mit 16 BundeslÃ¤ndern. Die Hauptstadt ist Berlin, und die Wirtschaft ist stark exportorientiert.
```

You can customize the input text directly inside `main.py` or extend the script for file/stream input.

<br>

# ğŸ› ï¸ Configuration

No external configuration is required. The summarizer uses the `facebook/bart-large-cnn` model by default and loads it directly via Hugging Face.

However, for advanced users:
- You can change the model by modifying the `pipeline(...)` call in `main.py`.
- You can also adjust `max_length`, `min_length`, or `do_sample` parameters.

<br>

# ğŸ“¦ Installation

## ğŸ³ Docker

Build and run the project in a container:

```bash
docker build -t llm-summarizer .
docker run --rm llm-summarizer
```

## ğŸ Poetry + Python

1. Install dependencies:

   ```bash
   poetry install
   ```

2. Run summarizer:

   ```bash
   poetry run python summarizer/main.py
   ```

<br>

# ğŸ“š Model Details

- **Model used:** `facebook/bart-large-cnn`
- **Architecture:** Encoder-Decoder (Seq2Seq Transformer)
- **Provider:** Hugging Face `transformers`
- **Summary behavior:** Non-sampling, deterministic output (via greedy decoding)

This model was trained on large news corpora (CNN/DailyMail) and is highly optimized for summarization tasks.

<br>

# ğŸš€ Summary

âœ… **Simple LLM summarization via Python or Docker**  
âœ… **Uses Hugging Face's BART for state-of-the-art output**  
âœ… **Extensible base for future LLM-driven text analysis**  
âœ… **Portable: local + containerized setup supported**  
